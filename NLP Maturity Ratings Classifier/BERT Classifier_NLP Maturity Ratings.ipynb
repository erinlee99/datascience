{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/AP/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYm2QjD4WPA"
      },
      "source": [
        "BERT for binary or multiclass document classification using the [CLS] token as the document representation; trains a model (on `train.txt`), uses `dev.txt` for early stopping, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals.\n",
        "\n",
        "Before executing this notebook on Colab, make sure you're running on cuda (`Runtime > Change runtime type > GPU`) to make use of GPU speedups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RXwcKMY44a04",
        "outputId": "c421226a-e9ad-49cb-f020-810da7c1db7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 21.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 71.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oeSgrDjF4WPC"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-ycKOkx34WPE",
        "outputId": "151b5c02-7516-484a-bc4e-fa00eaad8cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BuzjCRrG4WPF"
      },
      "outputs": [],
      "source": [
        "def read_labels(filename):\n",
        "    labels={}\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[1]\n",
        "            if label not in labels:\n",
        "                labels[label]=len(labels)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8XetKMAw4WPF"
      },
      "outputs": [],
      "source": [
        "def read_data(filename, labels, max_data_points=1000):\n",
        "  \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[1]\n",
        "            text = cols[2]\n",
        "            \n",
        "            data.append(text)\n",
        "            data_labels.append(labels[label])\n",
        "            \n",
        "\n",
        "    # shuffle the data\n",
        "    tmp = list(zip(data, data_labels))\n",
        "    random.shuffle(tmp)\n",
        "    data, data_labels = zip(*tmp)\n",
        "    \n",
        "    if max_data_points is None:\n",
        "        return data, data_labels\n",
        "    \n",
        "    return data[:max_data_points], data_labels[:max_data_points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZPfJqVmb4WPH"
      },
      "outputs": [],
      "source": [
        "labels=read_labels(\"s/train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O-728X0b4WPI"
      },
      "outputs": [],
      "source": [
        "train_x, train_y=read_data(\"s/train.txt\", labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zQOE469I4WPI"
      },
      "outputs": [],
      "source": [
        "dev_x, dev_y=read_data(\"s/dev.txt\", labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vMpv6ZV54WPI"
      },
      "outputs": [],
      "source": [
        "test_x, test_y=read_data(\"s/test.txt\", labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gUKRjWsf4WPJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, x, y):\n",
        "    model.eval()\n",
        "    corr = 0.\n",
        "    total = 0.\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(x, y):\n",
        "            y_preds=model.forward(x)\n",
        "            for idx, y_pred in enumerate(y_preds):\n",
        "                prediction=torch.argmax(y_pred)\n",
        "                if prediction == y[idx]:\n",
        "                    corr += 1.\n",
        "                total+=1                          \n",
        "    return corr/total, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xkUSuG794WPK"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model_name, params):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.model_name=bert_model_name\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n",
        "        self.bert = BertModel.from_pretrained(self.model_name)\n",
        "        \n",
        "        self.num_labels = params[\"label_length\"]\n",
        "\n",
        "        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
        "\n",
        "    def get_batches(self, all_x, all_y, batch_size=32, max_toks=510):\n",
        "            \n",
        "        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n",
        "      (and limited to a maximum number of WordPiece tokens \"\"\"\n",
        "\n",
        "        batches_x=[]\n",
        "        batches_y=[]\n",
        "        \n",
        "        for i in range(0, len(all_x), batch_size):\n",
        "\n",
        "            current_batch=[]\n",
        "\n",
        "            x=all_x[i:i+batch_size]\n",
        "\n",
        "            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
        "            batch_y=all_y[i:i+batch_size]\n",
        "\n",
        "            batches_x.append(batch_x.to(device))\n",
        "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
        "            \n",
        "        return batches_x, batches_y\n",
        "  \n",
        "\n",
        "    def forward(self, batch_x): \n",
        "    \n",
        "        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
        "                         attention_mask=batch_x[\"attention_mask\"],\n",
        "                         token_type_ids=batch_x[\"token_type_ids\"],\n",
        "                         output_hidden_states=True)\n",
        "\n",
        "      # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n",
        "      # And use the *last* layer output (layer -1)\n",
        "      # as a result of this choice, this embedding will be optimized for this purpose during the training process.\n",
        "      \n",
        "        bert_hidden_states = bert_output['hidden_states']\n",
        "\n",
        "        out = bert_hidden_states[-1][:,0,:]\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9mVrY9yv4WPL"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GiiXWwz94WPL"
      },
      "outputs": [],
      "source": [
        "def train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None, bs=32):\n",
        "\n",
        "    bert_model = BERTClassifier(bert_model_name, params={\"label_length\": len(labels), \"doLowerCase\":doLowerCase, \"embedding_size\":embedding_size})\n",
        "    bert_model.to(device)\n",
        "\n",
        "    batch_x, batch_y = bert_model.get_batches(train_x, train_y, batch_size=bs)\n",
        "    dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y, batch_size=bs)\n",
        "\n",
        "    optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs=30\n",
        "    best_dev_acc = 0.\n",
        "    patience=5\n",
        "\n",
        "    best_epoch=0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        bert_model.train()\n",
        "\n",
        "        # Train\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            y_pred = bert_model.forward(x)\n",
        "            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        dev_accuracy, _=evaluate(bert_model, dev_batch_x, dev_batch_y)\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "            if dev_accuracy > best_dev_acc:\n",
        "                torch.save(bert_model.state_dict(), model_filename)\n",
        "                best_dev_acc = dev_accuracy\n",
        "                best_epoch=epoch\n",
        "        if epoch - best_epoch > patience:\n",
        "            print(\"No improvement in dev accuracy over %s epochs; stopping training\" % patience)\n",
        "            break\n",
        "\n",
        "    bert_model.load_state_dict(torch.load(model_filename))\n",
        "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
        "    return bert_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Twbsysmd4WPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4561996d-9382-47f7-dcaf-93d644b109c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.505\n",
            "Epoch 1, dev accuracy: 0.505\n",
            "Epoch 2, dev accuracy: 0.505\n",
            "Epoch 3, dev accuracy: 0.520\n",
            "Epoch 4, dev accuracy: 0.515\n",
            "Epoch 5, dev accuracy: 0.515\n",
            "Epoch 6, dev accuracy: 0.515\n",
            "Epoch 7, dev accuracy: 0.505\n",
            "Epoch 8, dev accuracy: 0.505\n",
            "Epoch 9, dev accuracy: 0.520\n",
            "No improvement in dev accuracy over 5 epochs; stopping training\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.520\n"
          ]
        }
      ],
      "source": [
        "# small BERT -- can run on laptop\n",
        "# bert_model_name=\"google/bert_uncased_L-2_H-128_A-2\"\n",
        "# model_filename=\"mybert.model\"\n",
        "# embedding_size=128\n",
        "# doLowerCase=True\n",
        "\n",
        "# bert-base -- slow on laptop; better on Colab\n",
        "bert_model_name=\"bert-base-cased\"\n",
        "model_filename=\"mybert.model\"\n",
        "embedding_size=768\n",
        "doLowerCase=False\n",
        "\n",
        "model=train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UyImsx_C4WPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bf8be29e-1a21-4912-92bb-08350477d75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.482, 95% CIs: [0.413 0.552]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
        "accuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper=confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=True, bs = 64)\n",
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y, batch_size=64)\n",
        "accuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper=confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ],
      "metadata": {
        "id": "Um_mpAbVXSbN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "48179c14-4825-4b3d-b00a-fdd61d6a8efd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.505\n",
            "Epoch 1, dev accuracy: 0.505\n",
            "Epoch 2, dev accuracy: 0.505\n",
            "Epoch 3, dev accuracy: 0.505\n",
            "Epoch 4, dev accuracy: 0.515\n",
            "Epoch 5, dev accuracy: 0.495\n",
            "Epoch 6, dev accuracy: 0.490\n",
            "Epoch 7, dev accuracy: 0.505\n",
            "Epoch 8, dev accuracy: 0.510\n",
            "Epoch 9, dev accuracy: 0.490\n",
            "Epoch 10, dev accuracy: 0.500\n",
            "No improvement in dev accuracy over 5 epochs; stopping training\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.515\n",
            "Test accuracy for best dev model: 0.457, 95% CIs: [0.388 0.527]\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AP4_PartA_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}