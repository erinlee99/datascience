{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aId6ulmqgLoM"
      },
      "source": [
        "[Ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) is a classification method for categories on an ordinal scale -- e.g. [1, 2, 3, 4, 5] or [G, PG, PG-13, R].  This notebook implements ordinal regression using the method of [Frank and Hal 2001](https://www.cs.waikato.ac.nz/~eibe/pubs/ordinal_tech_report.pdf), which transforms a k-multiclass classifier into k-1 binary classifiers (each of which predicts whether a data point is above a threshold in the ordinal scale -- e.g., whether a movie is \"higher\" than PG).  This method can be used with any binary classification method that outputs probabilities; here L2-regularizaed binary logistic regression is used.\n",
        "\n",
        "This notebook trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "71852db7-7856-453e-f11d-8d64fd396016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SYlVkVbVgLoT"
      },
      "outputs": [],
      "source": [
        "def load_ordinal_data(filename, ordering):\n",
        "    X = []\n",
        "    Y = []\n",
        "    orig_Y=[]\n",
        "    for ordinal in ordering:\n",
        "        Y.append([])\n",
        "        \n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1].lstrip().rstrip()\n",
        "            text = cols[2]\n",
        "\n",
        "            X.append(text)\n",
        "            \n",
        "            index=ordering.index(label)\n",
        "            for i in range(len(ordering)):\n",
        "                if index > i:\n",
        "                    Y[i].append(1)\n",
        "                else:\n",
        "                    Y[i].append(0)\n",
        "            orig_Y.append(label)\n",
        "                    \n",
        "    return X, Y, orig_Y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gbJW_8eTgLoV"
      },
      "outputs": [],
      "source": [
        "class OrdinalClassifier:\n",
        "\n",
        "    def __init__(self, ordinal_values, feature_method, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY):\n",
        "        self.ordinal_values=ordinal_values\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_regs = [None]* (len(self.ordinal_values)-1)\n",
        "\n",
        "        self.trainY=trainY\n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "        \n",
        "        self.orig_trainY=orig_trainY\n",
        "        self.orig_devY=orig_devY\n",
        "        self.orig_testY=orig_testY\n",
        "        \n",
        "        self.trainX = self.process(trainX, training=True)\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "        \n",
        "        data = self.featurize(X_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "\n",
        "        \n",
        "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
        "            best_dev_accuracy=0\n",
        "            best_model=None\n",
        "            for C in [0.1, 1, 10, 100]:\n",
        "\n",
        "                log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "                log_reg.fit(self.trainX, self.trainY[idx])\n",
        "                development_accuracy = log_reg.score(self.devX, self.devY[idx])\n",
        "                if development_accuracy > best_dev_accuracy:\n",
        "                    best_dev_accuracy=development_accuracy\n",
        "                    best_model=log_reg\n",
        "\n",
        "\n",
        "            self.log_regs[idx]=best_model\n",
        "        \n",
        "    def test(self):\n",
        "        self.test_preds = []\n",
        "        self.test_actual = []\n",
        "        cor=tot=0\n",
        "        counts=Counter()\n",
        "        preds=[None]*(len(self.ordinal_values)-1)\n",
        "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
        "            preds[idx]=self.log_regs[idx].predict_proba(self.testX)[:,1]\n",
        "        \n",
        "        preds=np.array(preds)\n",
        "        \n",
        "            \n",
        "        for data_point in range(len(preds[0])):\n",
        "            \n",
        "    \n",
        "            ordinal_preds=np.zeros(len(self.ordinal_values))\n",
        "            for ordinal in range(len(self.ordinal_values)-1):\n",
        "                if ordinal == 0:\n",
        "                    ordinal_preds[ordinal]=1-preds[ordinal][data_point]\n",
        "                else:\n",
        "                    ordinal_preds[ordinal]=preds[ordinal-1][data_point]-preds[ordinal][data_point]\n",
        "\n",
        "            ordinal_preds[len(self.ordinal_values)-1]=preds[len(preds)-1][data_point]\n",
        "\n",
        "            prediction=np.argmax(ordinal_preds)\n",
        "            print(ordinal_preds, prediction)\n",
        "            counts[prediction]+=1\n",
        "            self.test_actual.append(self.ordinal_values.index(self.orig_testY[data_point]))\n",
        "            self.test_preds.append(prediction)\n",
        "            if prediction == self.ordinal_values.index(self.orig_testY[data_point]):\n",
        "                cor+=1\n",
        "            tot+=1\n",
        "\n",
        "        return cor/tot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wFmD-N8egLoX"
      },
      "outputs": [],
      "source": [
        "def binary_bow_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        feats[word]=1\n",
        "            \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JGx7pgrYgLoY"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TRa_FooagLoZ"
      },
      "outputs": [],
      "source": [
        "def run(trainingFile, devFile, testFile, ordinal_values):\n",
        "\n",
        "\n",
        "    trainX, trainY, orig_trainY=load_ordinal_data(trainingFile, ordinal_values)\n",
        "    devX, devY, orig_devY=load_ordinal_data(devFile, ordinal_values)\n",
        "    testX, testY, orig_testY=load_ordinal_data(testFile, ordinal_values)\n",
        "    \n",
        "    simple_classifier = OrdinalClassifier(ordinal_values, binary_bow_featurize, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "\n",
        "    lower, upper=confidence_intervals(accuracy, len(testY[0]), .95)\n",
        "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rQu-czv9gLoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d0630d66-027f-415e-ee63-ccd283312796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01934292 0.13786054 0.53888723 0.2598085  0.04410081] 2\n",
            "[ 0.01185623 -0.01064727  0.6567558   0.30190291  0.04013234] 2\n",
            "[ 0.0226212   0.1460621   0.76812383 -0.02456255  0.08775542] 2\n",
            "[0.01857329 0.05829023 0.75183423 0.14894179 0.02236045] 2\n",
            "[ 0.02106771 -0.00309937  0.19296053  0.77088716  0.01818397] 3\n",
            "[ 0.02086231  0.11480469  0.8463077  -0.03097514  0.04900044] 2\n",
            "[ 0.02228656  0.07670757  0.89198523 -0.04514012  0.05416077] 2\n",
            "[0.02231357 0.1844421  0.63135447 0.14472047 0.01716939] 2\n",
            "[ 0.01555949 -0.00888218  0.58595475  0.32456155  0.08280639] 2\n",
            "[0.01982086 0.3944889  0.22909925 0.33980601 0.01678497] 1\n",
            "[0.01183038 0.08524695 0.6501456  0.21766243 0.03511464] 2\n",
            "[0.01378647 0.100123   0.40642063 0.38732561 0.09234429] 2\n",
            "[0.0166261  0.01698383 0.77267374 0.16125335 0.03246297] 2\n",
            "[ 0.01571617  0.01767122  0.94863666 -0.0190051   0.03698106] 2\n",
            "[ 0.02018813  0.0416766   0.92921348 -0.00497081  0.0138926 ] 2\n",
            "[ 0.02303875  0.29776797  0.65347613 -0.0036904   0.02940756] 2\n",
            "[0.01645301 0.14901275 0.37336419 0.35936942 0.10180063] 2\n",
            "[0.02864752 0.06403325 0.74769149 0.10587315 0.0537546 ] 2\n",
            "[ 0.01144575 -0.001421    0.92450716  0.02145979  0.04400829] 2\n",
            "[0.02240939 0.3098609  0.26202226 0.35225352 0.05345392] 3\n",
            "[ 0.01640724 -0.00835072  0.59403876  0.34255321  0.05535152] 2\n",
            "[ 0.02914647  0.14518306  0.79450752 -0.0052725   0.03643545] 2\n",
            "[0.02833554 0.04299131 0.8456459  0.04198978 0.04103747] 2\n",
            "[ 0.01786997 -0.0072176   0.91802294  0.03892807  0.03239661] 2\n",
            "[0.02885349 0.43666051 0.40145406 0.11225822 0.02077372] 1\n",
            "[ 0.02879038 -0.01659239  0.89712975  0.04586596  0.0448063 ] 2\n",
            "[0.02635504 0.12458778 0.76900705 0.03222211 0.04782802] 2\n",
            "[ 0.01971605  0.407585    0.54989538 -0.01922899  0.04203256] 2\n",
            "[ 0.02159929  0.15233056  0.78560539 -0.0019654   0.04243016] 2\n",
            "[0.02107844 0.00348404 0.89329044 0.03495907 0.04718801] 2\n",
            "[0.0176032  0.07782598 0.83965765 0.04549462 0.01941854] 2\n",
            "[0.023676   0.07493879 0.66305902 0.2137182  0.02460799] 2\n",
            "[0.01355401 0.21137734 0.65448783 0.06496716 0.05561365] 2\n",
            "[ 0.01512308 -0.00365502  0.77137832  0.16602959  0.05112402] 2\n",
            "[0.01599292 0.00488321 0.37079721 0.56122192 0.04710473] 3\n",
            "[0.03241834 0.00370146 0.363489   0.53734655 0.06304465] 3\n",
            "[0.02235311 0.41617609 0.45277842 0.07286884 0.03582355] 2\n",
            "[0.02162356 0.46980248 0.38029336 0.0722384  0.05604219] 1\n",
            "[0.02434067 0.00957483 0.6828104  0.26163371 0.02164038] 2\n",
            "[0.01151632 0.01738794 0.63093178 0.27172439 0.06843957] 2\n",
            "[ 0.03038589  0.4526227   0.49513192 -0.00700042  0.02885992] 2\n",
            "[0.0309097  0.13300491 0.63475082 0.16900934 0.03232523] 2\n",
            "[ 0.01434966 -0.01072385  0.06846074  0.87963741  0.04827603] 3\n",
            "[0.026457   0.4019842  0.54536856 0.01493731 0.01125292] 2\n",
            "[ 0.01705073 -0.00506375  0.74511757  0.20657518  0.03632027] 2\n",
            "[0.01369398 0.24909118 0.16096782 0.55170941 0.02453762] 3\n",
            "[ 0.01864271 -0.00840709  0.17684739  0.77241736  0.04049963] 3\n",
            "[0.01324293 0.21992275 0.7094893  0.02447102 0.03287399] 2\n",
            "[0.01449639 0.31857568 0.55388564 0.10238842 0.01065387] 2\n",
            "[ 0.02528629  0.15699096  0.7617701  -0.00416191  0.06011456] 2\n",
            "[0.0231533  0.17403323 0.7688492  0.00132379 0.03264049] 2\n",
            "[0.02015726 0.15907486 0.48389996 0.29854555 0.03832238] 2\n",
            "[0.02604585 0.01189003 0.74876344 0.17813144 0.03516925] 2\n",
            "[0.0138668  0.09421514 0.62037347 0.24035281 0.03119178] 2\n",
            "[0.01251128 0.43024535 0.10799584 0.4169757  0.03227183] 1\n",
            "[0.01838889 0.17387714 0.76183145 0.03280181 0.01310072] 2\n",
            "[0.01676932 0.13752755 0.74941789 0.06048847 0.03579676] 2\n",
            "[0.01936053 0.00343038 0.73533164 0.19526782 0.04660963] 2\n",
            "[ 0.00918411 -0.00376918  0.77087162  0.19007392  0.03363954] 2\n",
            "[0.02964745 0.02666542 0.66526797 0.24558827 0.03283089] 2\n",
            "[0.02959166 0.07002984 0.34617377 0.51596237 0.03824235] 3\n",
            "[0.01152789 0.54663966 0.31293898 0.01998751 0.10890596] 1\n",
            "[ 0.00833764  0.01937164  0.96367322 -0.04000354  0.04862104] 2\n",
            "[0.02247517 0.09815654 0.72270815 0.13577162 0.02088852] 2\n",
            "[0.01022991 0.01959968 0.51066555 0.44091324 0.01859162] 2\n",
            "[0.01364675 0.00602429 0.22238949 0.71297845 0.04496103] 3\n",
            "[0.01302553 0.02251668 0.39695939 0.52989878 0.03759963] 3\n",
            "[ 0.01506633 -0.01058922  0.25026924  0.71370611  0.03154753] 3\n",
            "[0.01958485 0.03556974 0.66490237 0.25178468 0.02815836] 2\n",
            "[0.03496627 0.05357482 0.29843702 0.54690349 0.0661184 ] 3\n",
            "[ 0.02688459 -0.01206794  0.75528187  0.1953946   0.03450688] 2\n",
            "[0.03080854 0.0507216  0.79650025 0.09592161 0.02604799] 2\n",
            "[0.01706619 0.42559052 0.44924905 0.08365265 0.02444159] 2\n",
            "[0.02726105 0.16978883 0.75265202 0.01842154 0.03187655] 2\n",
            "[0.01322944 0.03849337 0.86413045 0.05337112 0.03077562] 2\n",
            "[0.02395191 0.16307825 0.61913519 0.16089626 0.03293839] 2\n",
            "[ 0.00988887 -0.00635747  0.13804362  0.80595704  0.05246794] 3\n",
            "[0.01370604 0.19833465 0.62171658 0.12043886 0.04580388] 2\n",
            "[0.01867781 0.01762039 0.67128406 0.26411033 0.0283074 ] 2\n",
            "[2.09808390e-02 1.14794008e-04 8.23342226e-01 1.11353148e-01\n",
            " 4.42089936e-02] 2\n",
            "[0.01702633 0.19374518 0.68617023 0.06560052 0.03745774] 2\n",
            "[0.03425462 0.03610812 0.90168707 0.00968518 0.01826501] 2\n",
            "[ 0.0223417  -0.01641954  0.66185133  0.30444416  0.02778236] 2\n",
            "[0.01170206 0.04172759 0.72714749 0.11808234 0.10134052] 2\n",
            "[3.63355565e-02 3.48835915e-03 9.22886032e-01 1.83809356e-04\n",
            " 3.71062426e-02] 2\n",
            "[0.01153516 0.0096127  0.48295333 0.44178134 0.05411747] 2\n",
            "[0.01987778 0.17790567 0.68156195 0.07930504 0.04134956] 2\n",
            "[ 0.01236511 -0.01015815  0.94429812  0.00601345  0.04748148] 2\n",
            "[0.02641862 0.01488354 0.71982269 0.19591152 0.04296364] 2\n",
            "[ 0.01395246 -0.00646875  0.97754812 -0.01026943  0.02523761] 2\n",
            "[ 0.02592977  0.36098691  0.57908428 -0.02488894  0.05888798] 2\n",
            "[0.01206702 0.00206099 0.52219568 0.38445791 0.07921841] 2\n",
            "[0.02138336 0.10766899 0.82545712 0.01567694 0.0298136 ] 2\n",
            "[ 0.0302795  -0.00879484  0.67407744  0.28312671  0.0213112 ] 2\n",
            "[0.02685441 0.19380795 0.10805759 0.62170493 0.04957512] 3\n",
            "[ 0.02929687  0.02693538  0.89654811 -0.01211514  0.05933478] 2\n",
            "[0.03323372 0.02166757 0.71494849 0.1931421  0.03700813] 2\n",
            "[0.02525822 0.06254387 0.70927274 0.13541006 0.06751511] 2\n",
            "[0.01616479 0.01377696 0.74705017 0.17626945 0.04673864] 2\n",
            "[ 0.01068629  0.04975848  0.93288731 -0.00985697  0.0165249 ] 2\n",
            "[0.03654272 0.27791977 0.54225186 0.11653791 0.02674773] 2\n",
            "[ 0.02491751  0.4972399   0.45414602 -0.02204681  0.04574338] 1\n",
            "[0.02548539 0.01918558 0.77910343 0.14257837 0.03364724] 2\n",
            "[0.02361927 0.38041459 0.51177075 0.04748924 0.03670616] 2\n",
            "[0.02220854 0.11468598 0.76963307 0.06870607 0.02476634] 2\n",
            "[0.01197854 0.22705246 0.68791722 0.05440874 0.01864303] 2\n",
            "[0.01522601 0.09683666 0.83374584 0.04041704 0.01377445] 2\n",
            "[0.02731117 0.14938091 0.77471793 0.02706412 0.02152587] 2\n",
            "[ 0.02016364 -0.00123713  0.46895469  0.47827604  0.03384277] 3\n",
            "[0.019269   0.18707181 0.74125218 0.02222631 0.0301807 ] 2\n",
            "[0.02030709 0.0009152  0.81697896 0.10744496 0.05435379] 2\n",
            "[ 0.01814931  0.06066983  0.9159552  -0.02675976  0.03198541] 2\n",
            "[0.02580581 0.04903576 0.6575237  0.24473362 0.02290111] 2\n",
            "[0.01995304 0.02363853 0.67659838 0.25860906 0.02120098] 2\n",
            "[ 0.02046589 -0.00086474  0.45713093  0.49697193  0.02629599] 3\n",
            "[0.01591363 0.31028844 0.55980884 0.07566239 0.03832669] 2\n",
            "[0.00870731 0.00187722 0.47023497 0.47457805 0.04460246] 3\n",
            "[ 0.02684288 -0.00715697  0.61362792  0.32472407  0.0419621 ] 2\n",
            "[0.01371273 0.08290898 0.82346466 0.06510712 0.01480651] 2\n",
            "[ 0.01518078 -0.00758747  0.81611653  0.14988829  0.02640188] 2\n",
            "[0.01274324 0.01253783 0.11223367 0.8264018  0.03608346] 3\n",
            "[ 0.014678   -0.01228155  0.37826516  0.59347716  0.02586123] 3\n",
            "[ 0.02052837 -0.01807147  0.4230483   0.52794498  0.04654982] 3\n",
            "[0.0296821  0.06600321 0.30579546 0.56419045 0.03432879] 3\n",
            "[ 0.02737009  0.30241709  0.65987502 -0.02812549  0.0384633 ] 2\n",
            "[0.02272172 0.00983575 0.82747218 0.09626666 0.04370368] 2\n",
            "[0.02084592 0.18555545 0.73137125 0.03463365 0.02759373] 2\n",
            "[ 0.00971208 -0.00653263  0.22573736  0.73146941  0.03961378] 3\n",
            "[0.01944095 0.08287429 0.86480236 0.00143312 0.03144928] 2\n",
            "[0.03354987 0.21037643 0.60589333 0.12731484 0.02286553] 2\n",
            "[0.02228237 0.03068091 0.74741654 0.16817225 0.03144793] 2\n",
            "[0.00940697 0.14318738 0.48154215 0.30975743 0.05610607] 2\n",
            "[ 0.01465169 -0.01132405  0.70835975  0.21705294  0.07125967] 2\n",
            "[ 0.02320555  0.02747079  0.88020382 -0.00501139  0.07413123] 2\n",
            "[ 0.01934315 -0.0046235   0.12218855  0.77844545  0.08464635] 3\n",
            "[0.02508475 0.01208706 0.48991905 0.4448486  0.02806054] 2\n",
            "[ 0.02071289 -0.00428124 -0.00630763  0.94421561  0.04566037] 3\n",
            "[0.02824848 0.55381245 0.39273901 0.00765644 0.01754362] 1\n",
            "[0.01622534 0.0389765  0.4715796  0.43514164 0.03807692] 2\n",
            "[0.04062776 0.0456504  0.61339228 0.24340229 0.05692727] 2\n",
            "[0.01148923 0.03106646 0.82944359 0.11544083 0.01255989] 2\n",
            "[ 0.02472383  0.02319335  0.94068724 -0.01191353  0.02330911] 2\n",
            "[0.01823475 0.56639854 0.37319578 0.01760928 0.02456165] 1\n",
            "[0.03029421 0.40449335 0.18255624 0.36322368 0.01943252] 1\n",
            "[0.01334978 0.06592743 0.90156638 0.00159145 0.01756496] 2\n",
            "[ 0.0174816   0.01643847  0.94858331 -0.00616789  0.02366451] 2\n",
            "[ 2.17860056e-02  1.22361409e-01  8.12935379e-01 -1.63649367e-04\n",
            "  4.30808557e-02] 2\n",
            "[0.01173936 0.01266396 0.53594307 0.41183795 0.02781566] 2\n",
            "[ 0.01221128 -0.00244409  0.87220308  0.06254273  0.05548699] 2\n",
            "[0.01653387 0.01546912 0.58804142 0.33524243 0.04471316] 2\n",
            "[ 0.01362463 -0.00407325  0.14340423  0.81893673  0.02810766] 3\n",
            "[0.0152827  0.06349377 0.58376205 0.29501768 0.04244379] 2\n",
            "[0.01827191 0.04872124 0.73273367 0.14294068 0.05733251] 2\n",
            "[0.02148077 0.27106088 0.54710115 0.11001752 0.05033968] 2\n",
            "[ 0.01362054  0.02286545  0.94432172 -0.05676618  0.07595847] 2\n",
            "[ 0.02807591 -0.01902059  0.96824839  0.00275526  0.01994103] 2\n",
            "[0.01881085 0.07723498 0.72082953 0.15453199 0.02859265] 2\n",
            "[ 0.01392661 -0.00724599  0.42053396  0.53002677  0.04275865] 3\n",
            "[ 0.01399948 -0.00838458  0.11512503  0.81082914  0.06843094] 3\n",
            "[0.01080975 0.01345634 0.91316319 0.03959283 0.0229779 ] 2\n",
            "[0.00864098 0.03810843 0.07434972 0.854337   0.02456387] 3\n",
            "[0.02044736 0.5660289  0.35979055 0.02422681 0.02950638] 1\n",
            "[0.03753388 0.11126174 0.76444572 0.05445682 0.03230183] 2\n",
            "[0.02342827 0.20257209 0.70031719 0.01773233 0.05595012] 2\n",
            "[ 0.019387    0.07010111  0.9081846  -0.01207513  0.01440243] 2\n",
            "[ 0.0215499  -0.01735068  0.87669776  0.09429742  0.02480559] 2\n",
            "[0.02465354 0.11099054 0.65445861 0.18119892 0.02869839] 2\n",
            "[ 0.00829564 -0.00173082  0.24338223  0.67763978  0.07241316] 3\n",
            "[ 0.01465998  0.01322102  0.94292688 -0.00120369  0.03039581] 2\n",
            "[0.0180487  0.08495378 0.60532207 0.26115308 0.03052238] 2\n",
            "[0.02337808 0.09704149 0.72434474 0.13578973 0.01944594] 2\n",
            "[0.01612749 0.21953151 0.72216128 0.02365143 0.01852829] 2\n",
            "[0.01840023 0.12895766 0.75157141 0.05436584 0.04670486] 2\n",
            "[0.01442202 0.00977528 0.11727546 0.80866648 0.04986075] 3\n",
            "[0.01329702 0.00399038 0.52110844 0.41612785 0.04547631] 2\n",
            "[ 0.0111299  -0.00474054  0.31894369  0.59198188  0.08268507] 3\n",
            "[ 0.01910019  0.17904066  0.77933997 -0.01289834  0.03541752] 2\n",
            "[0.0176304  0.24662822 0.70677474 0.00167162 0.02729502] 2\n",
            "[0.01362622 0.03186807 0.20088986 0.66840634 0.08520951] 3\n",
            "[0.01449288 0.04423935 0.18425125 0.70097533 0.05604119] 3\n",
            "[ 0.02127137 -0.00369853  0.78420414  0.13325715  0.06496586] 2\n",
            "[0.01158298 0.04236345 0.50323601 0.38781376 0.0550038 ] 2\n",
            "[0.01175802 0.01086713 0.60865747 0.3463378  0.02237959] 2\n",
            "[0.01263907 0.62945154 0.22120589 0.08710615 0.04959734] 1\n",
            "[0.01815233 0.03218414 0.69402768 0.20943921 0.04619665] 2\n",
            "[0.0109916  0.03126743 0.86236542 0.05309914 0.04227641] 2\n",
            "[ 0.01509172  0.28490399  0.66887012 -0.01577908  0.04691326] 2\n",
            "[0.01393081 0.25105269 0.57013223 0.13915831 0.02572596] 2\n",
            "[0.02064944 0.02563644 0.86381866 0.04375072 0.04614475] 2\n",
            "[0.03492575 0.36939344 0.54981235 0.01005565 0.03581281] 2\n",
            "[ 0.02325558 -0.00725503  0.78253224  0.16003464  0.04143257] 2\n",
            "[0.03894607 0.07587741 0.78760287 0.05311579 0.04445787] 2\n",
            "[0.02793008 0.60016488 0.24353988 0.07902753 0.04933764] 1\n",
            "[0.02384438 0.08083993 0.67770257 0.18250419 0.03510894] 2\n",
            "[ 0.01523495 -0.00467323  0.87876908  0.06878197  0.04188723] 2\n",
            "[0.01268223 0.02937647 0.90662731 0.03936413 0.01194986] 2\n",
            "[0.03064764 0.02640264 0.79648734 0.12335825 0.02310412] 2\n",
            "[ 0.01255076 -0.00395098  0.70973521  0.23404722  0.04761779] 2\n",
            "[ 0.03163303  0.48052872  0.4740742  -0.04762606  0.06139011] 1\n",
            "Test accuracy for best dev model: 0.472, 95% CIs: [0.403 0.542]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gid=2\n",
        "trainingFile = \"splits/%s/train.txt\" % gid\n",
        "devFile = \"splits/%s/dev.txt\" % gid\n",
        "testFile = \"splits/%s/test.txt\" % gid\n",
        "    \n",
        "# ordinal values must be in order *as strings* from smallest to largest, e.g.:\n",
        "ordinal_values=[\"G\", \"PG\", \"PG-13\", \"R\", \"NC-17\"]\n",
        "\n",
        "#ordinal_values=[\"0\", \"1\", \"2\"]\n",
        "\n",
        "run(trainingFile, devFile, testFile, ordinal_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part b) analysis**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3j8bqFtelL72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainX, trainY, orig_trainY=load_ordinal_data(trainingFile, ordinal_values)\n",
        "devX, devY, orig_devY=load_ordinal_data(devFile, ordinal_values)\n",
        "testX, testY, orig_testY=load_ordinal_data(testFile, ordinal_values)\n",
        "\n",
        "simple_classifier = OrdinalClassifier(ordinal_values, binary_bow_featurize, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n",
        "simple_classifier.train()\n",
        "simple_classifier.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "23q2LtreNVgf",
        "outputId": "3db9e8ed-0a05-45ef-9c94-c7e640afae4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01934292 0.13786054 0.53888723 0.2598085  0.04410081] 2\n",
            "[ 0.01185623 -0.01064727  0.6567558   0.30190291  0.04013234] 2\n",
            "[ 0.0226212   0.1460621   0.76812383 -0.02456255  0.08775542] 2\n",
            "[0.01857329 0.05829023 0.75183423 0.14894179 0.02236045] 2\n",
            "[ 0.02106771 -0.00309937  0.19296053  0.77088716  0.01818397] 3\n",
            "[ 0.02086231  0.11480469  0.8463077  -0.03097514  0.04900044] 2\n",
            "[ 0.02228656  0.07670757  0.89198523 -0.04514012  0.05416077] 2\n",
            "[0.02231357 0.1844421  0.63135447 0.14472047 0.01716939] 2\n",
            "[ 0.01555949 -0.00888218  0.58595475  0.32456155  0.08280639] 2\n",
            "[0.01982086 0.3944889  0.22909925 0.33980601 0.01678497] 1\n",
            "[0.01183038 0.08524695 0.6501456  0.21766243 0.03511464] 2\n",
            "[0.01378647 0.100123   0.40642063 0.38732561 0.09234429] 2\n",
            "[0.0166261  0.01698383 0.77267374 0.16125335 0.03246297] 2\n",
            "[ 0.01571617  0.01767122  0.94863666 -0.0190051   0.03698106] 2\n",
            "[ 0.02018813  0.0416766   0.92921348 -0.00497081  0.0138926 ] 2\n",
            "[ 0.02303875  0.29776797  0.65347613 -0.0036904   0.02940756] 2\n",
            "[0.01645301 0.14901275 0.37336419 0.35936942 0.10180063] 2\n",
            "[0.02864752 0.06403325 0.74769149 0.10587315 0.0537546 ] 2\n",
            "[ 0.01144575 -0.001421    0.92450716  0.02145979  0.04400829] 2\n",
            "[0.02240939 0.3098609  0.26202226 0.35225352 0.05345392] 3\n",
            "[ 0.01640724 -0.00835072  0.59403876  0.34255321  0.05535152] 2\n",
            "[ 0.02914647  0.14518306  0.79450752 -0.0052725   0.03643545] 2\n",
            "[0.02833554 0.04299131 0.8456459  0.04198978 0.04103747] 2\n",
            "[ 0.01786997 -0.0072176   0.91802294  0.03892807  0.03239661] 2\n",
            "[0.02885349 0.43666051 0.40145406 0.11225822 0.02077372] 1\n",
            "[ 0.02879038 -0.01659239  0.89712975  0.04586596  0.0448063 ] 2\n",
            "[0.02635504 0.12458778 0.76900705 0.03222211 0.04782802] 2\n",
            "[ 0.01971605  0.407585    0.54989538 -0.01922899  0.04203256] 2\n",
            "[ 0.02159929  0.15233056  0.78560539 -0.0019654   0.04243016] 2\n",
            "[0.02107844 0.00348404 0.89329044 0.03495907 0.04718801] 2\n",
            "[0.0176032  0.07782598 0.83965765 0.04549462 0.01941854] 2\n",
            "[0.023676   0.07493879 0.66305902 0.2137182  0.02460799] 2\n",
            "[0.01355401 0.21137734 0.65448783 0.06496716 0.05561365] 2\n",
            "[ 0.01512308 -0.00365502  0.77137832  0.16602959  0.05112402] 2\n",
            "[0.01599292 0.00488321 0.37079721 0.56122192 0.04710473] 3\n",
            "[0.03241834 0.00370146 0.363489   0.53734655 0.06304465] 3\n",
            "[0.02235311 0.41617609 0.45277842 0.07286884 0.03582355] 2\n",
            "[0.02162356 0.46980248 0.38029336 0.0722384  0.05604219] 1\n",
            "[0.02434067 0.00957483 0.6828104  0.26163371 0.02164038] 2\n",
            "[0.01151632 0.01738794 0.63093178 0.27172439 0.06843957] 2\n",
            "[ 0.03038589  0.4526227   0.49513192 -0.00700042  0.02885992] 2\n",
            "[0.0309097  0.13300491 0.63475082 0.16900934 0.03232523] 2\n",
            "[ 0.01434966 -0.01072385  0.06846074  0.87963741  0.04827603] 3\n",
            "[0.026457   0.4019842  0.54536856 0.01493731 0.01125292] 2\n",
            "[ 0.01705073 -0.00506375  0.74511757  0.20657518  0.03632027] 2\n",
            "[0.01369398 0.24909118 0.16096782 0.55170941 0.02453762] 3\n",
            "[ 0.01864271 -0.00840709  0.17684739  0.77241736  0.04049963] 3\n",
            "[0.01324293 0.21992275 0.7094893  0.02447102 0.03287399] 2\n",
            "[0.01449639 0.31857568 0.55388564 0.10238842 0.01065387] 2\n",
            "[ 0.02528629  0.15699096  0.7617701  -0.00416191  0.06011456] 2\n",
            "[0.0231533  0.17403323 0.7688492  0.00132379 0.03264049] 2\n",
            "[0.02015726 0.15907486 0.48389996 0.29854555 0.03832238] 2\n",
            "[0.02604585 0.01189003 0.74876344 0.17813144 0.03516925] 2\n",
            "[0.0138668  0.09421514 0.62037347 0.24035281 0.03119178] 2\n",
            "[0.01251128 0.43024535 0.10799584 0.4169757  0.03227183] 1\n",
            "[0.01838889 0.17387714 0.76183145 0.03280181 0.01310072] 2\n",
            "[0.01676932 0.13752755 0.74941789 0.06048847 0.03579676] 2\n",
            "[0.01936053 0.00343038 0.73533164 0.19526782 0.04660963] 2\n",
            "[ 0.00918411 -0.00376918  0.77087162  0.19007392  0.03363954] 2\n",
            "[0.02964745 0.02666542 0.66526797 0.24558827 0.03283089] 2\n",
            "[0.02959166 0.07002984 0.34617377 0.51596237 0.03824235] 3\n",
            "[0.01152789 0.54663966 0.31293898 0.01998751 0.10890596] 1\n",
            "[ 0.00833764  0.01937164  0.96367322 -0.04000354  0.04862104] 2\n",
            "[0.02247517 0.09815654 0.72270815 0.13577162 0.02088852] 2\n",
            "[0.01022991 0.01959968 0.51066555 0.44091324 0.01859162] 2\n",
            "[0.01364675 0.00602429 0.22238949 0.71297845 0.04496103] 3\n",
            "[0.01302553 0.02251668 0.39695939 0.52989878 0.03759963] 3\n",
            "[ 0.01506633 -0.01058922  0.25026924  0.71370611  0.03154753] 3\n",
            "[0.01958485 0.03556974 0.66490237 0.25178468 0.02815836] 2\n",
            "[0.03496627 0.05357482 0.29843702 0.54690349 0.0661184 ] 3\n",
            "[ 0.02688459 -0.01206794  0.75528187  0.1953946   0.03450688] 2\n",
            "[0.03080854 0.0507216  0.79650025 0.09592161 0.02604799] 2\n",
            "[0.01706619 0.42559052 0.44924905 0.08365265 0.02444159] 2\n",
            "[0.02726105 0.16978883 0.75265202 0.01842154 0.03187655] 2\n",
            "[0.01322944 0.03849337 0.86413045 0.05337112 0.03077562] 2\n",
            "[0.02395191 0.16307825 0.61913519 0.16089626 0.03293839] 2\n",
            "[ 0.00988887 -0.00635747  0.13804362  0.80595704  0.05246794] 3\n",
            "[0.01370604 0.19833465 0.62171658 0.12043886 0.04580388] 2\n",
            "[0.01867781 0.01762039 0.67128406 0.26411033 0.0283074 ] 2\n",
            "[2.09808390e-02 1.14794008e-04 8.23342226e-01 1.11353148e-01\n",
            " 4.42089936e-02] 2\n",
            "[0.01702633 0.19374518 0.68617023 0.06560052 0.03745774] 2\n",
            "[0.03425462 0.03610812 0.90168707 0.00968518 0.01826501] 2\n",
            "[ 0.0223417  -0.01641954  0.66185133  0.30444416  0.02778236] 2\n",
            "[0.01170206 0.04172759 0.72714749 0.11808234 0.10134052] 2\n",
            "[3.63355565e-02 3.48835915e-03 9.22886032e-01 1.83809356e-04\n",
            " 3.71062426e-02] 2\n",
            "[0.01153516 0.0096127  0.48295333 0.44178134 0.05411747] 2\n",
            "[0.01987778 0.17790567 0.68156195 0.07930504 0.04134956] 2\n",
            "[ 0.01236511 -0.01015815  0.94429812  0.00601345  0.04748148] 2\n",
            "[0.02641862 0.01488354 0.71982269 0.19591152 0.04296364] 2\n",
            "[ 0.01395246 -0.00646875  0.97754812 -0.01026943  0.02523761] 2\n",
            "[ 0.02592977  0.36098691  0.57908428 -0.02488894  0.05888798] 2\n",
            "[0.01206702 0.00206099 0.52219568 0.38445791 0.07921841] 2\n",
            "[0.02138336 0.10766899 0.82545712 0.01567694 0.0298136 ] 2\n",
            "[ 0.0302795  -0.00879484  0.67407744  0.28312671  0.0213112 ] 2\n",
            "[0.02685441 0.19380795 0.10805759 0.62170493 0.04957512] 3\n",
            "[ 0.02929687  0.02693538  0.89654811 -0.01211514  0.05933478] 2\n",
            "[0.03323372 0.02166757 0.71494849 0.1931421  0.03700813] 2\n",
            "[0.02525822 0.06254387 0.70927274 0.13541006 0.06751511] 2\n",
            "[0.01616479 0.01377696 0.74705017 0.17626945 0.04673864] 2\n",
            "[ 0.01068629  0.04975848  0.93288731 -0.00985697  0.0165249 ] 2\n",
            "[0.03654272 0.27791977 0.54225186 0.11653791 0.02674773] 2\n",
            "[ 0.02491751  0.4972399   0.45414602 -0.02204681  0.04574338] 1\n",
            "[0.02548539 0.01918558 0.77910343 0.14257837 0.03364724] 2\n",
            "[0.02361927 0.38041459 0.51177075 0.04748924 0.03670616] 2\n",
            "[0.02220854 0.11468598 0.76963307 0.06870607 0.02476634] 2\n",
            "[0.01197854 0.22705246 0.68791722 0.05440874 0.01864303] 2\n",
            "[0.01522601 0.09683666 0.83374584 0.04041704 0.01377445] 2\n",
            "[0.02731117 0.14938091 0.77471793 0.02706412 0.02152587] 2\n",
            "[ 0.02016364 -0.00123713  0.46895469  0.47827604  0.03384277] 3\n",
            "[0.019269   0.18707181 0.74125218 0.02222631 0.0301807 ] 2\n",
            "[0.02030709 0.0009152  0.81697896 0.10744496 0.05435379] 2\n",
            "[ 0.01814931  0.06066983  0.9159552  -0.02675976  0.03198541] 2\n",
            "[0.02580581 0.04903576 0.6575237  0.24473362 0.02290111] 2\n",
            "[0.01995304 0.02363853 0.67659838 0.25860906 0.02120098] 2\n",
            "[ 0.02046589 -0.00086474  0.45713093  0.49697193  0.02629599] 3\n",
            "[0.01591363 0.31028844 0.55980884 0.07566239 0.03832669] 2\n",
            "[0.00870731 0.00187722 0.47023497 0.47457805 0.04460246] 3\n",
            "[ 0.02684288 -0.00715697  0.61362792  0.32472407  0.0419621 ] 2\n",
            "[0.01371273 0.08290898 0.82346466 0.06510712 0.01480651] 2\n",
            "[ 0.01518078 -0.00758747  0.81611653  0.14988829  0.02640188] 2\n",
            "[0.01274324 0.01253783 0.11223367 0.8264018  0.03608346] 3\n",
            "[ 0.014678   -0.01228155  0.37826516  0.59347716  0.02586123] 3\n",
            "[ 0.02052837 -0.01807147  0.4230483   0.52794498  0.04654982] 3\n",
            "[0.0296821  0.06600321 0.30579546 0.56419045 0.03432879] 3\n",
            "[ 0.02737009  0.30241709  0.65987502 -0.02812549  0.0384633 ] 2\n",
            "[0.02272172 0.00983575 0.82747218 0.09626666 0.04370368] 2\n",
            "[0.02084592 0.18555545 0.73137125 0.03463365 0.02759373] 2\n",
            "[ 0.00971208 -0.00653263  0.22573736  0.73146941  0.03961378] 3\n",
            "[0.01944095 0.08287429 0.86480236 0.00143312 0.03144928] 2\n",
            "[0.03354987 0.21037643 0.60589333 0.12731484 0.02286553] 2\n",
            "[0.02228237 0.03068091 0.74741654 0.16817225 0.03144793] 2\n",
            "[0.00940697 0.14318738 0.48154215 0.30975743 0.05610607] 2\n",
            "[ 0.01465169 -0.01132405  0.70835975  0.21705294  0.07125967] 2\n",
            "[ 0.02320555  0.02747079  0.88020382 -0.00501139  0.07413123] 2\n",
            "[ 0.01934315 -0.0046235   0.12218855  0.77844545  0.08464635] 3\n",
            "[0.02508475 0.01208706 0.48991905 0.4448486  0.02806054] 2\n",
            "[ 0.02071289 -0.00428124 -0.00630763  0.94421561  0.04566037] 3\n",
            "[0.02824848 0.55381245 0.39273901 0.00765644 0.01754362] 1\n",
            "[0.01622534 0.0389765  0.4715796  0.43514164 0.03807692] 2\n",
            "[0.04062776 0.0456504  0.61339228 0.24340229 0.05692727] 2\n",
            "[0.01148923 0.03106646 0.82944359 0.11544083 0.01255989] 2\n",
            "[ 0.02472383  0.02319335  0.94068724 -0.01191353  0.02330911] 2\n",
            "[0.01823475 0.56639854 0.37319578 0.01760928 0.02456165] 1\n",
            "[0.03029421 0.40449335 0.18255624 0.36322368 0.01943252] 1\n",
            "[0.01334978 0.06592743 0.90156638 0.00159145 0.01756496] 2\n",
            "[ 0.0174816   0.01643847  0.94858331 -0.00616789  0.02366451] 2\n",
            "[ 2.17860056e-02  1.22361409e-01  8.12935379e-01 -1.63649367e-04\n",
            "  4.30808557e-02] 2\n",
            "[0.01173936 0.01266396 0.53594307 0.41183795 0.02781566] 2\n",
            "[ 0.01221128 -0.00244409  0.87220308  0.06254273  0.05548699] 2\n",
            "[0.01653387 0.01546912 0.58804142 0.33524243 0.04471316] 2\n",
            "[ 0.01362463 -0.00407325  0.14340423  0.81893673  0.02810766] 3\n",
            "[0.0152827  0.06349377 0.58376205 0.29501768 0.04244379] 2\n",
            "[0.01827191 0.04872124 0.73273367 0.14294068 0.05733251] 2\n",
            "[0.02148077 0.27106088 0.54710115 0.11001752 0.05033968] 2\n",
            "[ 0.01362054  0.02286545  0.94432172 -0.05676618  0.07595847] 2\n",
            "[ 0.02807591 -0.01902059  0.96824839  0.00275526  0.01994103] 2\n",
            "[0.01881085 0.07723498 0.72082953 0.15453199 0.02859265] 2\n",
            "[ 0.01392661 -0.00724599  0.42053396  0.53002677  0.04275865] 3\n",
            "[ 0.01399948 -0.00838458  0.11512503  0.81082914  0.06843094] 3\n",
            "[0.01080975 0.01345634 0.91316319 0.03959283 0.0229779 ] 2\n",
            "[0.00864098 0.03810843 0.07434972 0.854337   0.02456387] 3\n",
            "[0.02044736 0.5660289  0.35979055 0.02422681 0.02950638] 1\n",
            "[0.03753388 0.11126174 0.76444572 0.05445682 0.03230183] 2\n",
            "[0.02342827 0.20257209 0.70031719 0.01773233 0.05595012] 2\n",
            "[ 0.019387    0.07010111  0.9081846  -0.01207513  0.01440243] 2\n",
            "[ 0.0215499  -0.01735068  0.87669776  0.09429742  0.02480559] 2\n",
            "[0.02465354 0.11099054 0.65445861 0.18119892 0.02869839] 2\n",
            "[ 0.00829564 -0.00173082  0.24338223  0.67763978  0.07241316] 3\n",
            "[ 0.01465998  0.01322102  0.94292688 -0.00120369  0.03039581] 2\n",
            "[0.0180487  0.08495378 0.60532207 0.26115308 0.03052238] 2\n",
            "[0.02337808 0.09704149 0.72434474 0.13578973 0.01944594] 2\n",
            "[0.01612749 0.21953151 0.72216128 0.02365143 0.01852829] 2\n",
            "[0.01840023 0.12895766 0.75157141 0.05436584 0.04670486] 2\n",
            "[0.01442202 0.00977528 0.11727546 0.80866648 0.04986075] 3\n",
            "[0.01329702 0.00399038 0.52110844 0.41612785 0.04547631] 2\n",
            "[ 0.0111299  -0.00474054  0.31894369  0.59198188  0.08268507] 3\n",
            "[ 0.01910019  0.17904066  0.77933997 -0.01289834  0.03541752] 2\n",
            "[0.0176304  0.24662822 0.70677474 0.00167162 0.02729502] 2\n",
            "[0.01362622 0.03186807 0.20088986 0.66840634 0.08520951] 3\n",
            "[0.01449288 0.04423935 0.18425125 0.70097533 0.05604119] 3\n",
            "[ 0.02127137 -0.00369853  0.78420414  0.13325715  0.06496586] 2\n",
            "[0.01158298 0.04236345 0.50323601 0.38781376 0.0550038 ] 2\n",
            "[0.01175802 0.01086713 0.60865747 0.3463378  0.02237959] 2\n",
            "[0.01263907 0.62945154 0.22120589 0.08710615 0.04959734] 1\n",
            "[0.01815233 0.03218414 0.69402768 0.20943921 0.04619665] 2\n",
            "[0.0109916  0.03126743 0.86236542 0.05309914 0.04227641] 2\n",
            "[ 0.01509172  0.28490399  0.66887012 -0.01577908  0.04691326] 2\n",
            "[0.01393081 0.25105269 0.57013223 0.13915831 0.02572596] 2\n",
            "[0.02064944 0.02563644 0.86381866 0.04375072 0.04614475] 2\n",
            "[0.03492575 0.36939344 0.54981235 0.01005565 0.03581281] 2\n",
            "[ 0.02325558 -0.00725503  0.78253224  0.16003464  0.04143257] 2\n",
            "[0.03894607 0.07587741 0.78760287 0.05311579 0.04445787] 2\n",
            "[0.02793008 0.60016488 0.24353988 0.07902753 0.04933764] 1\n",
            "[0.02384438 0.08083993 0.67770257 0.18250419 0.03510894] 2\n",
            "[ 0.01523495 -0.00467323  0.87876908  0.06878197  0.04188723] 2\n",
            "[0.01268223 0.02937647 0.90662731 0.03936413 0.01194986] 2\n",
            "[0.03064764 0.02640264 0.79648734 0.12335825 0.02310412] 2\n",
            "[ 0.01255076 -0.00395098  0.70973521  0.23404722  0.04761779] 2\n",
            "[ 0.03163303  0.48052872  0.4740742  -0.04762606  0.06139011] 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4723618090452261"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "predictions = simple_classifier.test_preds\n",
        "predictions[0] = 0\n",
        "predictions[4] = 0\n",
        "cm = confusion_matrix(simple_classifier.test_actual, predictions, labels=[0, 1, 2, 3, 4])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"G\", \"PG\", \"PG-13\", \"R\", \"NC-17\"])\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lD8uODi7lUqi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "be9362b6-25d1-4915-d1ec-e064f2705654"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B)"
      ],
      "metadata": {
        "id": "lb6z181oS_ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**\n",
        "\n",
        "\n",
        "When creating our confusion matrix, we ran into errors in the beginning saying that the predicted labels and the true labels were not matching. When we debugged the error to see what the issue was, we found that none of the predicted labels included “G” and “NC-17” classifications. Because these categories did not exist at all in the predictions, there was a mismatch in the number of categories and thus, the confusion matrix could not be constructed. We manually added these categories with “predictions[0] = 0” and “predictions[4] = 0”, representing the categories “G” and “NC-17”, respectively. As a result, we got the following confusion matrix:\n",
        "\n",
        "\n",
        "\n",
        "From the above confusion matrix, we can interpret that the “PG-13” category has the largest proportion of correctly classified predicted labels, at a scale level 77. We can also interpret that the data is unbalanced as it is very concentrated in the “PG”, “PG-13”, and “R” categories. Also, these three categories are the ones that are often misclassified as one another. Specifically, “R” is misclassified as “”PG-13”, at scale level 40. ”PG” is often incorrectly classified as “PG-13”, at scale level 29. From this, we interpreted that the differentiating factors between these categories may have been hard to define for everyone’s human judgment, and that our guidelines for differentiating between these three categories should have been more clear. By better clarifying the specific conditions for each category, our team would be able to avoid our current misclassification challenge. \n",
        "\n",
        "<br> \n",
        "\n",
        "**Unbalanced Data**\n",
        "\n",
        "As mentioned previously our categories are extremely unbalanced. While Random oversampling might help, it's possible that our guidelines should be revised in the future, knowing what we know now, to include fewer, more even, categories. The ordinal classifier might benefit from adding features with weights to the PG and R categories as they were very often incorrectly classified, though it’s important to consider bias created from them. It’s Possible that the G and NC-17 labels might be better off ignored since the hypothetical context of people looking for specific levels of horror, it’s unlikely that someone looking for something in the R range would be so put off by a story that hasn’t already been removed by the subreddit’s guidelines, and someone looking for stories in the G label would also be fine with most PG examples.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Bias**\n",
        "\n",
        "Our guidelines largely were adapted from the MPAA rating system, but were also informed by our (and future annotators) own opinions on what crosses certain boundaries. We were very detailed but as there are many subjective criteria such as “profane language,” as well as shortness of text selections lending themselves to concepts and events being alluded to rather than explicit , there are many areas where unintentional bias might show up. Especially since the MPAA ratings themselves aren’t free from bias related to the people and environment that created them, for example female nudity being more accepted than male nudity, it's all the more important that there are ample narrators of diverse backgrounds, and perhaps sensitivity advising on the guidelines. Certain situations, as well, might be perceived as more dangerous by certain groups, further justifying the former point.\n",
        "\n",
        "Specifically for language, what counts as swearing, and at what point profane language might be “bad enough” to be given an R rating rather than PG-13, is very strongly influenced by the region and dialects of the people who created the guidelines and even the annotators who read each passage. Some regions might use certain swears more often than others, and so certain words might get a more taboo rating than was intended. It’s also impossible to include all swears in a list so it’s possible that slurs that are less well known especially by non-minority groups could end up classified as “for all ages.” \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Best Performing Model**\n",
        "\n",
        "Our best performing model was Bert, specifically where the batch size was increased. Increasing the Batch size prevents overfitting as the text passages are short. The model had .585 accuracy with .492 test accuracy.\n"
      ],
      "metadata": {
        "id": "tuYswqbbO5ny"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AP4_PartA&B_Ordinal Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}